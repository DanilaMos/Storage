{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanilaMos/Storage/blob/main/Assignment_01/UQ_CIFAR-10N_Ensembling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Uncertainty Quantification with CIFAR-10N and Ensembling\n",
        "By *First name* *Second name*.\n",
        "\n",
        "*Month, Day, 2025.*"
      ],
      "metadata": {
        "id": "dviRg5xn5mza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement"
      ],
      "metadata": {
        "id": "fS8QkXU47uNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-annotated versions of the CIFAR-10 and CIFAR-100 data which contains real-world human annotation errors. We show how these noise patterns deviate from the classically assumed ones and what the new challenges are. The website of CIFAR-N is available at [cifar-10-100n\n",
        "](https://github.com/UCSC-REAL/cifar-10-100n/tree/main) project."
      ],
      "metadata": {
        "id": "EvP6IWSB5s_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation of simulation models"
      ],
      "metadata": {
        "id": "kN9yU_U6766c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Install Libraries"
      ],
      "metadata": {
        "id": "9mpwlpoX5TI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMPnV0nV4_-I",
        "outputId": "fa939a89-362b-4648-f5ac-45772e6ce9ff",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting clearml\n",
            "  Downloading clearml-1.17.1-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (25.1.0)\n",
            "Collecting furl>=2.0.0 (from clearml)\n",
            "  Downloading furl-2.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from clearml) (1.26.4)\n",
            "Collecting pathlib2>=2.3.0 (from clearml)\n",
            "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (11.1.0)\n",
            "Requirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.11/dist-packages (from clearml) (5.9.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from clearml) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.32.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.3.0)\n",
            "Collecting pyjwt<2.10.0,>=2.4.0 (from clearml)\n",
            "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: referencing<0.40 in /usr/local/lib/python3.11/dist-packages (from clearml) (0.36.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.12)\n",
            "Collecting orderedmultidict>=1.0.1 (from furl>=2.0.0->clearml)\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->clearml) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->clearml) (0.22.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->clearml) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->clearml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->clearml) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading clearml-1.17.1-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\n",
            "Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pyjwt, pathlib2, orderedmultidict, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, furl, nvidia-cusolver-cu12, clearml, torchmetrics, pytorch-lightning\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.10.1\n",
            "    Uninstalling PyJWT-2.10.1:\n",
            "      Successfully uninstalled PyJWT-2.10.1\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed clearml-1.17.1 furl-2.1.3 lightning-utilities-0.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 orderedmultidict-1.0.1 pathlib2-2.3.7.post1 pyjwt-2.9.0 pytorch-lightning-2.5.0.post0 torchmetrics-1.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning clearml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pytorch modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import datasets, transforms\n",
        "#scipy\n",
        "from scipy.stats import mode\n",
        "#sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "#Numpy\n",
        "import numpy as np\n",
        "#Pandas\n",
        "import pandas as pd\n",
        "#Lightning & logging\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "#Data observation\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import requests\n",
        "from pathlib import Path\n",
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Logging\n",
        "from clearml import Task"
      ],
      "metadata": {
        "id": "fDHrafErmo43"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the Models"
      ],
      "metadata": {
        "id": "ealb85K93wDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulation Settings"
      ],
      "metadata": {
        "id": "GHIKBWI93-zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the current directory"
      ],
      "metadata": {
        "id": "kiYPAzh54gjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd() #returns the current working directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "phE7U1vu31BR",
        "outputId": "ba3b610c-df04-4b9f-9221-e26de758f63d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/\")\n",
        "print(f'CHECKPOINT_PATH: {CHECKPOINT_PATH}')\n",
        "\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EVwTLDiNYyc",
        "outputId": "add3789f-0b04-48d2-be73-2e502a07a3b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECKPOINT_PATH: saved_models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the reproducibility options"
      ],
      "metadata": {
        "id": "3WK77wcO6sfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for setting the seed to implement parallel tests\n",
        "SEEDS = [42, 0, 17, 9, 3, 16, 2]\n",
        "SEED = 42 # random seed by default\n",
        "pl.seed_everything(SEED)\n",
        "\n",
        "# Determine the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prioritizes speed but may reduce precision\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# # Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "# torch.backends.cudnn.deterministic = True\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "# torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# torch.manual_seed(SEED)\n",
        "# np.random.seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YX7JeP93-TZ",
        "outputId": "5f373945-2fb0-463b-8411-4750f237fcfe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logging"
      ],
      "metadata": {
        "id": "_7ULmzow4jSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To configure ClearML in your Colab environment, follow these steps:\n",
        "\n",
        "---\n",
        "\n",
        "*Step 1: Create a ClearML Account*\n",
        "1. Go to the [ClearML website](https://clear.ml/).\n",
        "2. Sign up for a free account if you don’t already have one.\n",
        "3. Once registered, log in to your ClearML account.\n",
        "\n",
        "---\n",
        "\n",
        "*Step 2: Get Your ClearML Credentials*\n",
        "1. After logging in, navigate to the **Settings** page (click on your profile icon in the top-right corner and select **Settings**).\n",
        "2. Under the **Workspace** section, find your **+ Create new credentials**.\n",
        "3. Copy these credentials for a Jupiter notebook into the code cell below.\n",
        "\n",
        "---\n",
        "\n",
        "*Step 3: Accessing the ClearML Dashboard*\n",
        "1. Go to your ClearML dashboard (https://app.clear.ml).\n",
        "2. Navigate to the **Projects** section to see your experiments.\n",
        "3. Click on the experiment (e.g., `Lab_1`) to view detailed metrics, logs, and artifacts.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "C97DLT0gK37A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here to implement Step 2 of the logging instruction as it is shown below\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=ZTNOJ0LDBNTNHV6W9928PB4N6ENY4Y\n",
        "%env CLEARML_API_SECRET_KEY=23xgA8Vl1w6ehrDVDZycmY0QC4EVLE8W3W6Qn2vZGVDruDBkZL05rDhQIMZJVtj84nM"
      ],
      "metadata": {
        "id": "lTXMGNya32_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e02cc096-6909-416b-847f-69a6ef53ea5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=ZTNOJ0LDBNTNHV6W9928PB4N6ENY4Y\n",
            "env: CLEARML_API_SECRET_KEY=23xgA8Vl1w6ehrDVDZycmY0QC4EVLE8W3W6Qn2vZGVDruDBkZL05rDhQIMZJVtj84nM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "BujHK4sw7cA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary"
      ],
      "metadata": {
        "id": "Wb0uJtxz-E--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'CIFAR10N' # dataset with the real-world noise\n",
        "# Can be 'clean_label', 'worse_label', 'aggre_label', 'random_label1', 'random_label2', 'random_label3'\n",
        "NOISE_TYPE = 'worse_label'\n",
        "\n",
        "NS = {\n",
        "    'train': 45000,\n",
        "    'val': 5000,\n",
        "    'test': 10000\n",
        "} # for MNIST\n",
        "\n",
        "SIZE = 32 #image size\n",
        "NUM_CLASSES = 10\n",
        "CLASS_NAMES = ['plane', 'car', 'bird', 'cat',\n",
        "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "hWRDBJbO7k_u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization parameters"
      ],
      "metadata": {
        "id": "qP6TSQ-Z-Hxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For the CIFAR-10 dataset\n",
        "MEAN = np.array([0.491,0.482,0.447])\n",
        "STD  = np.array([0.247,0.243,0.261])"
      ],
      "metadata": {
        "id": "Oh-UxEQ3-Le3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforms"
      ],
      "metadata": {
        "id": "StCJNi9PDVZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collect parameters"
      ],
      "metadata": {
        "id": "GwaBDKEvD5ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model parameters\n",
        "LOSS_FUN = 'CE' # 'CE','CELoss'(custom), 'N', 'B', etc.\n",
        "ARCHITECTURE = 'CNN' # 'CNN, 'ResNet50', 'ViT', etc.\n",
        "\n",
        "#Collect the parameters (hyperparams and others)\n",
        "hparams = {\n",
        "    \"seed\": SEED,\n",
        "    \"lr\": 0.001,\n",
        "    'weight_decay': 0.0,\n",
        "    \"dropout\": 0.0,\n",
        "    \"bs\": 128,\n",
        "    \"num_workers\": 0, #set 2 in Colab, or 0 in InnoDataHub\n",
        "    \"num_epochs\": 20,\n",
        "    \"criterion\": LOSS_FUN,\n",
        "    \"architecture\": ARCHITECTURE,\n",
        "    \"num_samples\": NS,\n",
        "    \"im_size\": SIZE,\n",
        "    \"mean\": np.array([0.4914, 0.4822, 0.4465]),\n",
        "    \"std\": np.array([0.2470, 0.2435, 0.2616]),\n",
        "    'randResCrop': {'size': (SIZE, SIZE), 'scale': (0.8, 1.0), 'ratio': (0.9, 1.1)},\n",
        "    \"n_classes\": NUM_CLASSES,\n",
        "    \"noise_path\": './data/CIFAR-10_human.pt',\n",
        "    \"noise_type\": NOISE_TYPE  # Can be 'clean_label', 'worse_label', 'aggre_label', etc.\n",
        "}\n",
        "\n",
        "#Visualization\n",
        "vis_params = {\n",
        "    'fig_size': 5,\n",
        "    'num_samples': 5,\n",
        "    'num_bins': 50,\n",
        "}"
      ],
      "metadata": {
        "id": "NSyUzYCSD0v3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "OQsu5FcbFfwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightning"
      ],
      "metadata": {
        "id": "ZAOMnXlqFofC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data module"
      ],
      "metadata": {
        "id": "2N2vslXcUvGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, save_path):\n",
        "    \"\"\"Download a file from a URL and save it to the specified path.\"\"\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure directory exists\n",
        "        with open(save_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"File downloaded and saved to {save_path}\")\n",
        "    else:\n",
        "        raise Exception(f\"Failed to download file from {url}. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "l4LT-NyRgAWZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10(datasets.CIFAR10):\n",
        "    \"\"\"CIFAR10 dataset with noisy labels.\"\"\"\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None,\n",
        "                 download=False, noise_type=None, noise_path=None, is_human=True):\n",
        "        super().__init__(root, train=train, transform=transform,\n",
        "                         target_transform=target_transform, download=download)\n",
        "        self.noise_type = noise_type\n",
        "        self.noise_path = noise_path\n",
        "        self.is_human = is_human\n",
        "\n",
        "        if self.train and self.noise_type is not None:\n",
        "            self.load_noisy_labels()\n",
        "\n",
        "    def load_noisy_labels(self):\n",
        "        noise_file = torch.load(self.noise_path)\n",
        "        if isinstance(noise_file, dict):\n",
        "            if \"clean_label\" in noise_file.keys():\n",
        "                clean_label = torch.tensor(noise_file['clean_label'])\n",
        "                assert torch.sum(torch.tensor(self.targets) - clean_label) == 0\n",
        "                print(f'Loaded {self.noise_type} from {self.noise_path}.')\n",
        "                print(f'The overall noise rate is {1 - np.mean(clean_label.numpy() == noise_file[self.noise_type])}')\n",
        "            self.noisy_labels = noise_file[self.noise_type].reshape(-1)\n",
        "        else:\n",
        "            raise Exception('Input Error')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = super().__getitem__(index)\n",
        "        if self.train and self.noise_type is not None:\n",
        "            target = self.noisy_labels[index]\n",
        "        return img, target, index"
      ],
      "metadata": {
        "id": "_3lxFLivgJIn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.seed = params['seed']\n",
        "        self.batch_size = params['bs']\n",
        "        self.num_workers = params['num_workers']\n",
        "        self.mean = params['mean']\n",
        "        self.std = params['std']\n",
        "        self.ns = params['num_samples']\n",
        "        self.rand_res_crop = params['randResCrop']\n",
        "        self.noise_path = params.get('noise_path', './data/CIFAR-10_human.pt')\n",
        "        self.noise_type = params.get('noise_type', 'worse_label')  # Default to 'worse_label'\n",
        "\n",
        "        # Ensure the data directory exists\n",
        "        os.makedirs(os.path.dirname(self.noise_path), exist_ok=True)\n",
        "\n",
        "        # Download the CIFAR-10_human.pt file if it doesn't exist\n",
        "        if not os.path.exists(self.noise_path):\n",
        "            print(f\"Downloading CIFAR-10_human.pt from GitHub...\")\n",
        "            download_file(\n",
        "                url=\"https://github.com/UCSC-REAL/cifar-10-100n/raw/main/data/CIFAR-10_human.pt\",\n",
        "                save_path=self.noise_path\n",
        "            )\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(size=self.rand_res_crop['size'],\n",
        "                                         scale=self.rand_res_crop['scale'],\n",
        "                                         ratio=self.rand_res_crop['ratio']),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Download CIFAR-10 dataset\n",
        "        datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Load noisy labels\n",
        "        noise_file = torch.load(self.noise_path)\n",
        "        clean_label = noise_file['clean_label']\n",
        "        noisy_label = noise_file[self.noise_type]\n",
        "\n",
        "        # Split dataset into train and validation sets\n",
        "        cifar10_full = CIFAR10(root='./data', train=True, transform=self.transform,\n",
        "                               noise_type=self.noise_type, noise_path=self.noise_path, is_human=True, download=True)\n",
        "        pl.seed_everything(self.seed)\n",
        "        self.cifar10_train, self.cifar10_val = random_split(cifar10_full,\n",
        "                                                            [self.ns['train'],\n",
        "                                                             self.ns['val']])\n",
        "        self.cifar10_test = CIFAR10(root='./data', train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.cifar10_train, batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.cifar10_val, batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.cifar10_test, batch_size=self.batch_size,\n",
        "                          shuffle=False)"
      ],
      "metadata": {
        "id": "7WcQVIXnfiX3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training module"
      ],
      "metadata": {
        "id": "JBWSuYApFu7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class train_model(pl.LightningModule):\n",
        "    def __init__(self, model=None, loss=None, hparams=hparams):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "        self.model = model\n",
        "        self.loss_fn = loss\n",
        "        self.nc = hparams['n_classes']\n",
        "        self.lr = hparams['lr']\n",
        "        self.wd = hparams['weight_decay']\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, _ = batch  # Unpack batch (ignore indices for now)\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "\n",
        "        # Log training loss and accuracy\n",
        "        # preds = torch.argmax(logits[:, :self.nc], dim=1)\n",
        "        # acc = (preds == y).float().mean()\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        # self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, _ = batch  # Unpack batch (ignore indices for now)\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "\n",
        "        # Log validation loss and accuracy\n",
        "        # preds = torch.argmax(logits[:, :self.nc], dim=1)\n",
        "        # acc = (preds == y).float().mean()\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        # self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y, _ = batch  # Unpack batch (ignore indices for now)\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "\n",
        "        # Log test loss and accuracy\n",
        "        preds = torch.argmax(logits[:, :self.nc], dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return {'loss': loss, 'preds': preds, 'y': y}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
        "\n",
        "        # Optionally, add a learning rate scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.0)\n",
        "        return [optimizer], [scheduler]"
      ],
      "metadata": {
        "id": "g-uYzVgj2f-6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "SpGtoJicGCJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN from paper by [Xia](https://arxiv.org/abs/2106.00445)"
      ],
      "metadata": {
        "id": "aA10MARCuTL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy the code from the paper\n",
        "def call_bn(bn, x):\n",
        "    return bn(x)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_channel=3, n_outputs=10, dropout_rate=0.25, top_bn=False):\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.top_bn = top_bn\n",
        "        super(CNN, self).__init__()\n",
        "        self.c1=nn.Conv2d(input_channel,128,kernel_size=3,stride=1, padding=1)\n",
        "        self.c2=nn.Conv2d(128,128,kernel_size=3,stride=1, padding=1)\n",
        "        self.c3=nn.Conv2d(128,128,kernel_size=3,stride=1, padding=1)\n",
        "        self.c4=nn.Conv2d(128,256,kernel_size=3,stride=1, padding=1)\n",
        "        self.c5=nn.Conv2d(256,256,kernel_size=3,stride=1, padding=1)\n",
        "        self.c6=nn.Conv2d(256,256,kernel_size=3,stride=1, padding=1)\n",
        "        self.c7=nn.Conv2d(256,512,kernel_size=3,stride=1, padding=0)\n",
        "        self.c8=nn.Conv2d(512,256,kernel_size=3,stride=1, padding=0)\n",
        "        self.c9=nn.Conv2d(256,128,kernel_size=3,stride=1, padding=0)\n",
        "        self.l_c1=nn.Linear(128,n_outputs)\n",
        "        self.bn1=nn.BatchNorm2d(128)\n",
        "        self.bn2=nn.BatchNorm2d(128)\n",
        "        self.bn3=nn.BatchNorm2d(128)\n",
        "        self.bn4=nn.BatchNorm2d(256)\n",
        "        self.bn5=nn.BatchNorm2d(256)\n",
        "        self.bn6=nn.BatchNorm2d(256)\n",
        "        self.bn7=nn.BatchNorm2d(512)\n",
        "        self.bn8=nn.BatchNorm2d(256)\n",
        "        self.bn9=nn.BatchNorm2d(128)\n",
        "\n",
        "    def forward(self, x,):\n",
        "        h=x\n",
        "        h=self.c1(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn1, h), negative_slope=0.01)\n",
        "        h=self.c2(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn2, h), negative_slope=0.01)\n",
        "        h=self.c3(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn3, h), negative_slope=0.01)\n",
        "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
        "        h=F.dropout2d(h, p=self.dropout_rate)\n",
        "\n",
        "        h=self.c4(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn4, h), negative_slope=0.01)\n",
        "        h=self.c5(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn5, h), negative_slope=0.01)\n",
        "        h=self.c6(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn6, h), negative_slope=0.01)\n",
        "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
        "        h=F.dropout2d(h, p=self.dropout_rate)\n",
        "\n",
        "        h=self.c7(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn7, h), negative_slope=0.01)\n",
        "        h=self.c8(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn8, h), negative_slope=0.01)\n",
        "        h=self.c9(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn9, h), negative_slope=0.01)\n",
        "        h=F.avg_pool2d(h, kernel_size=h.data.shape[2])\n",
        "\n",
        "        h = h.view(h.size(0), h.size(1))\n",
        "        logit=self.l_c1(h)\n",
        "        if self.top_bn:\n",
        "            logit=call_bn(self.bn_c1, logit)\n",
        "        return logit"
      ],
      "metadata": {
        "id": "oA84H5z_t4oY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50"
      ],
      "metadata": {
        "id": "vOCLuzMRdgrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet50(nn.Module):\n",
        "    def __init__(self, n_outputs):\n",
        "        super(ResNet50, self).__init__()\n",
        "        self.n_outputs = n_outputs\n",
        "        # Define your ResNet50 layers here"
      ],
      "metadata": {
        "id": "jzeV5ga6df-q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT"
      ],
      "metadata": {
        "id": "eAY6gv8RdffZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, n_outputs):\n",
        "        super(ViT, self).__init__()\n",
        "        self.n_outputs = n_outputs\n",
        "        # Define your Vision Transformer layers here"
      ],
      "metadata": {
        "id": "EC11nnWZdfF4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss functions"
      ],
      "metadata": {
        "id": "rHm8wnbnGEnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a loss function class, or use a standart one."
      ],
      "metadata": {
        "id": "8z05aQ7cQTm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross entropy loss maden from scratch (just in case)\n",
        "class CELoss(nn.Module):\n",
        "    def __init__(self, reduction='mean'):\n",
        "        super(CELoss, self).__init__()\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Compute softmax probabilities\n",
        "        prob = nn.functional.softmax(x, 1)\n",
        "        # Compute log probabilities\n",
        "        log_prob = -1.0 * torch.log(prob)\n",
        "        # Gather the log probabilities for the true labels\n",
        "        loss = log_prob.gather(1, y.unsqueeze(1))\n",
        "        # Apply reduction\n",
        "        if self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif self.reduction == 'none':\n",
        "            loss = loss.squeeze()  # Remove extra dimension for consistency\n",
        "        else:\n",
        "            raise ValueError(\"Invalid reduction option.\")\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "8auVRUCKGEG2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BLoss(nn.Module): #NLoss\n",
        "    def __init__(self, label_smoothing=0.1, num_classes=NUM_CLASSES):\n",
        "        super(NLoss, self).__init__()\n",
        "        self.label_smoothing = label_smoothing\n",
        "        self.num_classes = num_classes\n",
        "        self.inv_smoothing = 1.0 - label_smoothing  # Probability for the correct class\n",
        "        self.smoothing = label_smoothing / (num_classes - 1)  # Probability for incorrect classes\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        x: Model output (logits + log variance)\n",
        "            - x[:, :self.num_classes]: Logits for class probabilities (h)\n",
        "            - x[:, self.num_classes:]: Logarithmic variance (s)\n",
        "        y: Labels\n",
        "        \"\"\"\n",
        "        # Split the model output into predictions (h) and log variance (s)\n",
        "        logits = x[:, :self.num_classes]  # Predictions (h)\n",
        "        log_var = x[:, self.num_classes:]  # Logarithmic variance (s)\n",
        "\n",
        "        # Apply label smoothing to the one-hot encoded labels\n",
        "        with torch.no_grad():\n",
        "            yoh = torch.zeros_like(logits)\n",
        "            yoh.fill_(self.smoothing / (self.num_classes - 1))\n",
        "            yoh.scatter_(1, y.data.unsqueeze(1), self.inv_smoothing)\n",
        "\n",
        "        # Compute the squared differences between predictions and smoothed labels\n",
        "        squared_diff = torch.pow(yoh - logits, 2)  # (y_k - h_k)^2\n",
        "\n",
        "        # Compute the exponential of the negative log variance (e^{-s})\n",
        "        exp_neg_log_var = torch.exp(-log_var)\n",
        "\n",
        "        # Compute the first term of the loss: e^{-s} * sum((y_k - h_k)^2)\n",
        "        term1 = exp_neg_log_var * squared_diff.sum(dim=1)\n",
        "\n",
        "        # Compute the second term of the loss: N * s\n",
        "        term2 = self.num_classes * log_var\n",
        "\n",
        "        # Combine the terms and compute the mean over the batch\n",
        "        loss = (term1 + term2).mean()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "An6Q9DUNK3SE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class BLoss(nn.Module):\n",
        "#     def __init__(self, label_smoothing=0.1, num_classes=NUM_CLASSES):\n",
        "#         super(BLoss, self).__init__()\n",
        "#         self.inv_smoothing = 1.0 - label_smoothing\n",
        "#         self.smoothing = label_smoothing\n",
        "#         self.num_classes = num_classes\n",
        "\n",
        "#     def forward(self, x, y):\n",
        "#         # Extract certainty and probabilities from the model output\n",
        "#         pass\n",
        "#         #Enter your code here"
      ],
      "metadata": {
        "id": "99a0Eh3AK5wT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models zoo"
      ],
      "metadata": {
        "id": "ZHSuQd99s_Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architectures and loss functions"
      ],
      "metadata": {
        "id": "T0QP_le1tPL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arch_and_loss(hparams):\n",
        "    \"\"\"\n",
        "    Returns the architecture and loss function based on the provided hparams.\n",
        "\n",
        "    Args:\n",
        "        hparams (dict): Hyperparameters dictionary, including 'ARCHITECTURE' and 'criterion'.\n",
        "\n",
        "    Returns:\n",
        "        arch: The model architecture.\n",
        "        loss: The loss function.\n",
        "    \"\"\"\n",
        "    # Determine the number of outputs based on the loss function\n",
        "    if hparams['criterion'] in ['B', 'N']:\n",
        "        n_outputs = hparams['n_classes'] + 1  # Add 1 output neuron for BLoss or NLoss\n",
        "    else:\n",
        "        n_outputs = hparams['n_classes']  # Default number of outputs\n",
        "\n",
        "    # Define the architectures\n",
        "    architectures = {\n",
        "        'CNN': CNN(n_outputs=n_outputs),\n",
        "        'ResNet50': ResNet50(n_outputs=n_outputs),\n",
        "        'ViT': ViT(n_outputs=n_outputs),\n",
        "    }\n",
        "\n",
        "    # Define the loss functions\n",
        "    losses = {\n",
        "        'CE': nn.CrossEntropyLoss(),\n",
        "        'B': BLoss(),\n",
        "        'N': NLoss(),\n",
        "    }\n",
        "\n",
        "    # Get the architecture and loss based on hparams\n",
        "    arch = architectures.get(hparams['architecture'])\n",
        "    loss = losses.get(hparams['criterion'])\n",
        "\n",
        "    if arch is None:\n",
        "        raise ValueError(f\"Architecture '{hparams['ARCHITECTURE']}' is not supported.\")\n",
        "    if loss is None:\n",
        "        raise ValueError(f\"Loss function '{hparams['criterion']}' is not supported.\")\n",
        "\n",
        "    return arch, loss\n"
      ],
      "metadata": {
        "id": "WfLgyavidUcW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "CWrc-f3sWDDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(dataloader,model,hparams=hparams,loss_fn_red=None):\n",
        "    # Collect images, predictions, and losses\n",
        "    # images = []\n",
        "    preds  = []\n",
        "    labels = []\n",
        "    losses = []\n",
        "    correct= 0\n",
        "    total  = 0\n",
        "    for batch in dataloader:\n",
        "        x, y, _ = batch\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            # loss = loss_fn_red(h,y)\n",
        "            pred = torch.argmax(logits[:,:hparams['n_classes']], dim=1)\n",
        "        correct += (pred == y).sum().item()  # Number of correct predictions\n",
        "        total += y.size(0)  # Total number of samples\n",
        "\n",
        "        # images.extend(x.cpu())\n",
        "        preds.extend(pred.cpu().numpy())\n",
        "        labels.extend(y.cpu().numpy())\n",
        "        # losses.extend(loss.cpu().numpy())\n",
        "    acc = correct / total\n",
        "    return preds, labels, acc"
      ],
      "metadata": {
        "id": "7793mMFKWFYS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensembling\n",
        "This approach is expected to give a robust ensemble model that leverages the diversity introduced by different seeds, potentially improving the overall accuracy on the test set."
      ],
      "metadata": {
        "id": "KemJG62D7EJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataset and Data Loaders"
      ],
      "metadata": {
        "id": "7d0C987gzzlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of the dataset, the dataloader, and the training module"
      ],
      "metadata": {
        "id": "tQmBPmYH21V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here\n",
        "data_module = CIFAR10DataModule(hparams)"
      ],
      "metadata": {
        "id": "jrjw6mU-z36-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2466575-0335-420a-e08c-c313f80d861e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CIFAR-10_human.pt from GitHub...\n",
            "File downloaded and saved to ./data/CIFAR-10_human.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_module.setup()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFq3JSDOJJs2",
        "outputId": "26c77d90-8155-412b-fa8f-434b821a03dc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-17aad7a7c416>:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  noise_file = torch.load(self.noise_path)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [04:39<00:00, 611kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-19c4723df6fe>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  noise_file = torch.load(self.noise_path)\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded worse_label from ./data/CIFAR-10_human.pt.\n",
            "The overall noise rate is 0.40208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(data_module.cifar10_test))"
      ],
      "metadata": {
        "id": "qqGF7jmOGmhY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Ensemble"
      ],
      "metadata": {
        "id": "oBfy6DZL0tii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop over different seeds"
      ],
      "metadata": {
        "id": "sjxsAXHbrGC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store predictions from each model\n",
        "all_predictions = []"
      ],
      "metadata": {
        "id": "4qZzt5w4smrt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in SEEDS:\n",
        "    # Set seed for reproducibility at the VERY BEGINNING\n",
        "    pl.seed_everything(seed)\n",
        "\n",
        "    # Reinitialize the model architecture for each seed\n",
        "    arch, loss_fn =  get_arch_and_loss(hparams)\n",
        "\n",
        "\n",
        "    checkpoint_callback_img = ModelCheckpoint(\n",
        "        monitor = 'val_loss',\n",
        "        dirpath = CHECKPOINT_PATH,\n",
        "        filename = f'best_model_{ARCHITECTURE}_{LOSS_FUN}_{seed}_{NOISE_TYPE}',\n",
        "        save_top_k = 1,\n",
        "        mode = 'min'\n",
        "    )\n",
        "\n",
        "    task = Task.init(project_name='CV-2025-CIFAR', task_name=f'arch_{ARCHITECTURE}_loss_{LOSS_FUN}_seed_{seed}_noise_{NOISE_TYPE}')\n",
        "\n",
        "    # Initialize the model with the reinitialized architecture\n",
        "    model = train_model(model=arch, loss=loss_fn)\n",
        "\n",
        "    # Log hyperparameters to ClearML\n",
        "    task.connect(model.hparams)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=hparams['num_epochs'],\n",
        "        callbacks=checkpoint_callback_img,\n",
        "        accelerator='auto',\n",
        "        devices='auto'\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, data_module)\n",
        "\n",
        "    best_model_path = checkpoint_callback_img.best_model_path\n",
        "    task.update_output_model(model_path=best_model_path, auto_delete_file=False)\n",
        "    best_model = train_model.load_from_checkpoint(best_model_path, model=model, loss=loss_fn)\n",
        "\n",
        "    # Test set\n",
        "    test_dataloader = data_module.test_dataloader()\n",
        "\n",
        "    # Move the model to the correct device\n",
        "    best_model = best_model.to(device)\n",
        "    predictions = []\n",
        "\n",
        "    if seed != SEEDS[-1]:\n",
        "        task.close()\n",
        "        del[model, best_model, task, arch, loss_fn]"
      ],
      "metadata": {
        "id": "QisO7wDarFVS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "881fea76-a874-46fd-b20f-f6808734ae45"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'NLoss' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c7eab1de3136>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Reinitialize the model architecture for each seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_arch_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-5dd97854b9da>\u001b[0m in \u001b[0;36mget_arch_and_loss\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     26\u001b[0m     losses = {\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m'CE'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;34m'B'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;34m'N'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     }\n",
            "\u001b[0;32m<ipython-input-18-839fea759533>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, label_smoothing, num_classes)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#NLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_smoothing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NLoss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the models and the ensemble of the models"
      ],
      "metadata": {
        "id": "NYbuTthVuCDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions"
      ],
      "metadata": {
        "id": "zOYyF657_kOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual models"
      ],
      "metadata": {
        "id": "yRxEnWceGYxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store individual model accuracies\n",
        "individual_accuracies = []\n",
        "\n",
        "# Compute accuracy for each model\n",
        "for i, predictions in enumerate(all_predictions):\n",
        "    # Get predictions for the current model\n",
        "    model_predictions = predictions  # Shape: (num_samples,)\n",
        "\n",
        "    # Get true labels (already collected earlier)\n",
        "    true_labels = np.array(data_module.cifar10_test.targets)\n",
        "\n",
        "    # Calculate accuracy for the current model\n",
        "    accuracy = accuracy_score(true_labels, model_predictions)\n",
        "    individual_accuracies.append(accuracy)\n",
        "    print(f'Model {i+1} Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Convert to numpy array for easier calculations\n",
        "individual_accuracies = np.array(individual_accuracies)\n",
        "\n",
        "# Compute mean accuracy\n",
        "mean_accuracy = np.mean(individual_accuracies)\n",
        "\n",
        "# Compute standard deviation of accuracy\n",
        "std_accuracy = np.std(individual_accuracies)\n",
        "\n",
        "print(f'Mean Accuracy: {mean_accuracy:.4f}')\n",
        "print(f'Standard Deviation of Accuracy: {std_accuracy:.4f}')"
      ],
      "metadata": {
        "id": "WSr0zvfIGeSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble"
      ],
      "metadata": {
        "id": "iop73XibGhUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack predictions from all models\n",
        "all_predictions = np.stack(all_predictions)  # Shape: (num_models, num_samples, num_classes)\n",
        "\n",
        "# Ensemble predictions (e.g., by averaging)\n",
        "ensemble_predictions = np.mean(all_predictions, axis=0)  # Shape: (num_samples, num_classes)\n",
        "final_predictions, _ = mode(all_predictions, axis=0)  # Majority voting\n",
        "final_predictions = final_predictions.flatten()  # Flatten to 1D array\n",
        "\n",
        "# Get true labels from the CIFAR-10 data set\n",
        "test_labels = np.array(data_module.cifar10_test.targets)\n",
        "# test_labels = data_module.test_dataset.labels  # Adjust this based on your dataset\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, final_predictions)\n",
        "print(f'Ensemble Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(test_labels, final_predictions)"
      ],
      "metadata": {
        "id": "xyUFGexws8QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated test metrics\n",
        "test_metrics = {\n",
        "    \"Mean Accuracy (individual)\": mean_accuracy,\n",
        "    \"Standard Deviation of Accuracy (individual)\": std_accuracy,\n",
        "    \"Ensemble Accuracy\": accuracy,\n",
        "}\n",
        "\n",
        "task.connect(test_metrics)"
      ],
      "metadata": {
        "id": "VbEGs97IgtRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task.close()"
      ],
      "metadata": {
        "id": "nIqc-cspappI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}