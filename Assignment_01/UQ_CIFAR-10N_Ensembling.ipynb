{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanilaMos/Storage/blob/main/Assignment_01/UQ_CIFAR-10N_Ensembling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Uncertainty Quantification with CIFAR-10N and Ensembling\n",
        "By *First name* *Second name*.\n",
        "\n",
        "*Month, Day, 2025.*"
      ],
      "metadata": {
        "id": "dviRg5xn5mza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement"
      ],
      "metadata": {
        "id": "fS8QkXU47uNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-annotated versions of the CIFAR-10 and CIFAR-100 data which contains real-world human annotation errors. We show how these noise patterns deviate from the classically assumed ones and what the new challenges are. The website of CIFAR-N is available at [cifar-10-100n\n",
        "](https://github.com/UCSC-REAL/cifar-10-100n/tree/main) project."
      ],
      "metadata": {
        "id": "EvP6IWSB5s_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation of simulation models"
      ],
      "metadata": {
        "id": "kN9yU_U6766c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Install Libraries"
      ],
      "metadata": {
        "id": "9mpwlpoX5TI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMPnV0nV4_-I",
        "outputId": "bd1a36e8-608e-47ee-df24-f3ddc6b847d3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.0.post0)\n",
            "Requirement already satisfied: clearml in /usr/local/lib/python3.11/dist-packages (1.17.1)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (25.1.0)\n",
            "Requirement already satisfied: furl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.1.3)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from clearml) (1.26.4)\n",
            "Requirement already satisfied: pathlib2>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.3.7.post1)\n",
            "Requirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (11.1.0)\n",
            "Requirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.11/dist-packages (from clearml) (5.9.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from clearml) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.32.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.3.0)\n",
            "Requirement already satisfied: pyjwt<2.10.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.9.0)\n",
            "Requirement already satisfied: referencing<0.40 in /usr/local/lib/python3.11/dist-packages (from clearml) (0.36.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.12)\n",
            "Requirement already satisfied: orderedmultidict>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from furl>=2.0.0->clearml) (1.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->clearml) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->clearml) (0.22.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->clearml) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->clearml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->clearml) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning clearml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pytorch modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import datasets, transforms\n",
        "#scipy\n",
        "from scipy.stats import mode\n",
        "#sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "#Numpy\n",
        "import numpy as np\n",
        "#Pandas\n",
        "import pandas as pd\n",
        "#Lightning & logging\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "#Data observation\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import requests\n",
        "from pathlib import Path\n",
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Logging\n",
        "from clearml import Task"
      ],
      "metadata": {
        "id": "fDHrafErmo43"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the Models"
      ],
      "metadata": {
        "id": "ealb85K93wDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulation Settings"
      ],
      "metadata": {
        "id": "GHIKBWI93-zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the current directory"
      ],
      "metadata": {
        "id": "kiYPAzh54gjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd() #returns the current working directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "phE7U1vu31BR",
        "outputId": "61fb14e0-44e1-4a34-a03f-5aeed52d7838"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/\")\n",
        "print(f'CHECKPOINT_PATH: {CHECKPOINT_PATH}')\n",
        "\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EVwTLDiNYyc",
        "outputId": "d6866b27-6bd9-4d27-eb03-24751cec08eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECKPOINT_PATH: saved_models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the reproducibility options"
      ],
      "metadata": {
        "id": "3WK77wcO6sfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for setting the seed to implement parallel tests\n",
        "SEEDS = [42, 0, 17, 9, 3, 16, 2]\n",
        "SEED = 42 # random seed by default\n",
        "pl.seed_everything(SEED)\n",
        "\n",
        "# Determine the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prioritizes speed but may reduce precision\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# # Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "# torch.backends.cudnn.deterministic = True\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "# torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# torch.manual_seed(SEED)\n",
        "# np.random.seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YX7JeP93-TZ",
        "outputId": "de78a70b-6897-42b4-e64c-cc8886be4b0c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logging"
      ],
      "metadata": {
        "id": "_7ULmzow4jSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To configure ClearML in your Colab environment, follow these steps:\n",
        "\n",
        "---\n",
        "\n",
        "*Step 1: Create a ClearML Account*\n",
        "1. Go to the [ClearML website](https://clear.ml/).\n",
        "2. Sign up for a free account if you donâ€™t already have one.\n",
        "3. Once registered, log in to your ClearML account.\n",
        "\n",
        "---\n",
        "\n",
        "*Step 2: Get Your ClearML Credentials*\n",
        "1. After logging in, navigate to the **Settings** page (click on your profile icon in the top-right corner and select **Settings**).\n",
        "2. Under the **Workspace** section, find your **+ Create new credentials**.\n",
        "3. Copy these credentials for a Jupiter notebook into the code cell below.\n",
        "\n",
        "---\n",
        "\n",
        "*Step 3: Accessing the ClearML Dashboard*\n",
        "1. Go to your ClearML dashboard (https://app.clear.ml).\n",
        "2. Navigate to the **Projects** section to see your experiments.\n",
        "3. Click on the experiment (e.g., `Lab_1`) to view detailed metrics, logs, and artifacts.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "C97DLT0gK37A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here to implement Step 2 of the logging instruction as it is shown below\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=ZTNOJ0LDBNTNHV6W9928PB4N6ENY4Y\n",
        "%env CLEARML_API_SECRET_KEY=23xgA8Vl1w6ehrDVDZycmY0QC4EVLE8W3W6Qn2vZGVDruDBkZL05rDhQIMZJVtj84nM"
      ],
      "metadata": {
        "id": "lTXMGNya32_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3246a9-1c8b-4f6e-bbd7-eb4fd2dd915e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=ZTNOJ0LDBNTNHV6W9928PB4N6ENY4Y\n",
            "env: CLEARML_API_SECRET_KEY=23xgA8Vl1w6ehrDVDZycmY0QC4EVLE8W3W6Qn2vZGVDruDBkZL05rDhQIMZJVtj84nM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "BujHK4sw7cA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary"
      ],
      "metadata": {
        "id": "Wb0uJtxz-E--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'CIFAR10N' # dataset with the real-world noise\n",
        "# Can be 'clean_label', 'worse_label', 'aggre_label', 'random_label1', 'random_label2', 'random_label3'\n",
        "NOISE_TYPE = 'worse_label'\n",
        "\n",
        "NS = {\n",
        "    'train': 45000,\n",
        "    'val': 5000,\n",
        "    'test': 10000\n",
        "} # for MNIST\n",
        "\n",
        "SIZE = 32 #image size\n",
        "NUM_CLASSES = 10\n",
        "CLASS_NAMES = ['plane', 'car', 'bird', 'cat',\n",
        "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "hWRDBJbO7k_u"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization parameters"
      ],
      "metadata": {
        "id": "qP6TSQ-Z-Hxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For the CIFAR-10 dataset\n",
        "MEAN = np.array([0.491,0.482,0.447])\n",
        "STD  = np.array([0.247,0.243,0.261])"
      ],
      "metadata": {
        "id": "Oh-UxEQ3-Le3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforms"
      ],
      "metadata": {
        "id": "StCJNi9PDVZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collect parameters"
      ],
      "metadata": {
        "id": "GwaBDKEvD5ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model parameters\n",
        "LOSS_FUN = 'CE' # 'CE','CELoss'(custom), 'N', 'B', etc.\n",
        "ARCHITECTURE = 'CNN' # 'CNN, 'ResNet50', 'ViT', etc.\n",
        "\n",
        "#Collect the parameters (hyperparams and others)\n",
        "hparams = {\n",
        "    \"seed\": SEED,\n",
        "    \"lr\": 0.001,\n",
        "    'weight_decay': 0.0,\n",
        "    \"dropout\": 0.0,\n",
        "    \"bs\": 128,\n",
        "    \"num_workers\": 0, #set 2 in Colab, or 0 in InnoDataHub\n",
        "    \"num_epochs\": 20,\n",
        "    \"criterion\": LOSS_FUN,\n",
        "    \"architecture\": ARCHITECTURE,\n",
        "    \"num_samples\": NS,\n",
        "    \"im_size\": SIZE,\n",
        "    \"mean\": np.array([0.4914, 0.4822, 0.4465]),\n",
        "    \"std\": np.array([0.2470, 0.2435, 0.2616]),\n",
        "    'randResCrop': {'size': (SIZE, SIZE), 'scale': (0.8, 1.0), 'ratio': (0.9, 1.1)},\n",
        "    \"n_classes\": NUM_CLASSES,\n",
        "    \"noise_path\": './data/CIFAR-10_human.pt',\n",
        "    \"noise_type\": NOISE_TYPE  # Can be 'clean_label', 'worse_label', 'aggre_label', etc.\n",
        "}\n",
        "\n",
        "#Visualization\n",
        "vis_params = {\n",
        "    'fig_size': 5,\n",
        "    'num_samples': 5,\n",
        "    'num_bins': 50,\n",
        "}"
      ],
      "metadata": {
        "id": "NSyUzYCSD0v3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "OQsu5FcbFfwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightning"
      ],
      "metadata": {
        "id": "ZAOMnXlqFofC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data module"
      ],
      "metadata": {
        "id": "2N2vslXcUvGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, save_path):\n",
        "    \"\"\"Download a file from a URL and save it to the specified path.\"\"\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure directory exists\n",
        "        with open(save_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"File downloaded and saved to {save_path}\")\n",
        "    else:\n",
        "        raise Exception(f\"Failed to download file from {url}. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "l4LT-NyRgAWZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10(datasets.CIFAR10):\n",
        "    \"\"\"CIFAR10 dataset with noisy labels.\"\"\"\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None,\n",
        "                 download=False, noise_type=None, noise_path=None, is_human=True):\n",
        "        super().__init__(root, train=train, transform=transform,\n",
        "                         target_transform=target_transform, download=download)\n",
        "        self.noise_type = noise_type\n",
        "        self.noise_path = noise_path\n",
        "        self.is_human = is_human\n",
        "\n",
        "        if self.train and self.noise_type is not None:\n",
        "            self.load_noisy_labels()\n",
        "\n",
        "    def load_noisy_labels(self):\n",
        "        noise_file = torch.load(self.noise_path)\n",
        "        if isinstance(noise_file, dict):\n",
        "            if \"clean_label\" in noise_file.keys():\n",
        "                clean_label = torch.tensor(noise_file['clean_label'])\n",
        "                assert torch.sum(torch.tensor(self.targets) - clean_label) == 0\n",
        "                print(f'Loaded {self.noise_type} from {self.noise_path}.')\n",
        "                print(f'The overall noise rate is {1 - np.mean(clean_label.numpy() == noise_file[self.noise_type])}')\n",
        "            self.noisy_labels = noise_file[self.noise_type].reshape(-1)\n",
        "        else:\n",
        "            raise Exception('Input Error')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = super().__getitem__(index)\n",
        "        if self.train and self.noise_type is not None:\n",
        "            target = self.noisy_labels[index]\n",
        "        return img, target, index"
      ],
      "metadata": {
        "id": "_3lxFLivgJIn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.seed = params['seed']\n",
        "        self.batch_size = params['bs']\n",
        "        self.num_workers = params['num_workers']\n",
        "        self.mean = params['mean']\n",
        "        self.std = params['std']\n",
        "        self.ns = params['num_samples']\n",
        "        self.rand_res_crop = params['randResCrop']\n",
        "        self.noise_path = params.get('noise_path', './data/CIFAR-10_human.pt')\n",
        "        self.noise_type = params.get('noise_type', 'worse_label')  # Default to 'worse_label'\n",
        "\n",
        "        # Ensure the data directory exists\n",
        "        os.makedirs(os.path.dirname(self.noise_path), exist_ok=True)\n",
        "\n",
        "        # Download the CIFAR-10_human.pt file if it doesn't exist\n",
        "        if not os.path.exists(self.noise_path):\n",
        "            print(f\"Downloading CIFAR-10_human.pt from GitHub...\")\n",
        "            download_file(\n",
        "                url=\"https://github.com/UCSC-REAL/cifar-10-100n/raw/main/data/CIFAR-10_human.pt\",\n",
        "                save_path=self.noise_path\n",
        "            )\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(size=self.rand_res_crop['size'],\n",
        "                                         scale=self.rand_res_crop['scale'],\n",
        "                                         ratio=self.rand_res_crop['ratio']),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Download CIFAR-10 dataset\n",
        "        datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Load noisy labels\n",
        "        noise_file = torch.load(self.noise_path)\n",
        "        clean_label = noise_file['clean_label']\n",
        "        noisy_label = noise_file[self.noise_type]\n",
        "\n",
        "        # Split dataset into train and validation sets\n",
        "        cifar10_full = CIFAR10(root='./data', train=True, transform=self.transform,\n",
        "                               noise_type=self.noise_type, noise_path=self.noise_path, is_human=True)\n",
        "        pl.seed_everything(self.seed)\n",
        "        self.cifar10_train, self.cifar10_val = random_split(cifar10_full,\n",
        "                                                            [self.ns['train'],\n",
        "                                                             self.ns['val']])\n",
        "        self.cifar10_test = CIFAR10(root='./data', train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.cifar10_train, batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.cifar10_val, batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.cifar10_test, batch_size=self.batch_size,\n",
        "                          shuffle=False)"
      ],
      "metadata": {
        "id": "7WcQVIXnfiX3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training module"
      ],
      "metadata": {
        "id": "JBWSuYApFu7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class train_model(pl.LightningModule):\n",
        "    def __init__(self, model=None, loss=None, hparams=hparams):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "        self.model = model\n",
        "        self.loss_fn = loss\n",
        "        self.nc = hparams['n_classes']\n",
        "        self.lr = hparams['lr']\n",
        "        self.wd = hparams['weight_decay']\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, _ = batch  # Unpack batch (ignore indices for now)\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "\n",
        "        # Log training loss and accuracy\n",
        "        # preds = torch.argmax(logits[:, :self.nc], dim=1)\n",
        "        # acc = (preds == y).float().mean()\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        # self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, _ = batch  # Unpack batch (ignore indices for now)\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "\n",
        "        # Log validation loss and accuracy\n",
        "        # preds = torch.argmax(logits[:, :self.nc], dim=1)\n",
        "        # acc = (preds == y).float().mean()\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        # self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y, _ = batch  # Unpack batch (ignore indices for now)\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "\n",
        "        # Log test loss and accuracy\n",
        "        preds = torch.argmax(logits[:, :self.nc], dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return {'loss': loss, 'preds': preds, 'y': y}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
        "\n",
        "        # Optionally, add a learning rate scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1.0)\n",
        "        return [optimizer], [scheduler]"
      ],
      "metadata": {
        "id": "g-uYzVgj2f-6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "SpGtoJicGCJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN from paper by [Xia](https://arxiv.org/abs/2106.00445)"
      ],
      "metadata": {
        "id": "aA10MARCuTL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy the code from the paper\n",
        "def call_bn(bn, x):\n",
        "    return bn(x)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_channel=3, n_outputs=10, dropout_rate=0.25, top_bn=False):\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.top_bn = top_bn\n",
        "        super(CNN, self).__init__()\n",
        "        self.c1=nn.Conv2d(input_channel,128,kernel_size=3,stride=1, padding=1)\n",
        "        self.c2=nn.Conv2d(128,128,kernel_size=3,stride=1, padding=1)\n",
        "        self.c3=nn.Conv2d(128,128,kernel_size=3,stride=1, padding=1)\n",
        "        self.c4=nn.Conv2d(128,256,kernel_size=3,stride=1, padding=1)\n",
        "        self.c5=nn.Conv2d(256,256,kernel_size=3,stride=1, padding=1)\n",
        "        self.c6=nn.Conv2d(256,256,kernel_size=3,stride=1, padding=1)\n",
        "        self.c7=nn.Conv2d(256,512,kernel_size=3,stride=1, padding=0)\n",
        "        self.c8=nn.Conv2d(512,256,kernel_size=3,stride=1, padding=0)\n",
        "        self.c9=nn.Conv2d(256,128,kernel_size=3,stride=1, padding=0)\n",
        "        self.l_c1=nn.Linear(128,n_outputs)\n",
        "        self.bn1=nn.BatchNorm2d(128)\n",
        "        self.bn2=nn.BatchNorm2d(128)\n",
        "        self.bn3=nn.BatchNorm2d(128)\n",
        "        self.bn4=nn.BatchNorm2d(256)\n",
        "        self.bn5=nn.BatchNorm2d(256)\n",
        "        self.bn6=nn.BatchNorm2d(256)\n",
        "        self.bn7=nn.BatchNorm2d(512)\n",
        "        self.bn8=nn.BatchNorm2d(256)\n",
        "        self.bn9=nn.BatchNorm2d(128)\n",
        "\n",
        "    def forward(self, x,):\n",
        "        h=x\n",
        "        h=self.c1(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn1, h), negative_slope=0.01)\n",
        "        h=self.c2(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn2, h), negative_slope=0.01)\n",
        "        h=self.c3(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn3, h), negative_slope=0.01)\n",
        "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
        "        h=F.dropout2d(h, p=self.dropout_rate)\n",
        "\n",
        "        h=self.c4(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn4, h), negative_slope=0.01)\n",
        "        h=self.c5(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn5, h), negative_slope=0.01)\n",
        "        h=self.c6(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn6, h), negative_slope=0.01)\n",
        "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
        "        h=F.dropout2d(h, p=self.dropout_rate)\n",
        "\n",
        "        h=self.c7(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn7, h), negative_slope=0.01)\n",
        "        h=self.c8(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn8, h), negative_slope=0.01)\n",
        "        h=self.c9(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn9, h), negative_slope=0.01)\n",
        "        h=F.avg_pool2d(h, kernel_size=h.data.shape[2])\n",
        "\n",
        "        h = h.view(h.size(0), h.size(1))\n",
        "        logit=self.l_c1(h)\n",
        "        if self.top_bn:\n",
        "            logit=call_bn(self.bn_c1, logit)\n",
        "        return logit"
      ],
      "metadata": {
        "id": "oA84H5z_t4oY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50"
      ],
      "metadata": {
        "id": "vOCLuzMRdgrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet50(nn.Module):\n",
        "    def __init__(self, n_outputs):\n",
        "        super(ResNet50, self).__init__()\n",
        "        self.n_outputs = n_outputs\n",
        "        # Define your ResNet50 layers here"
      ],
      "metadata": {
        "id": "jzeV5ga6df-q"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT"
      ],
      "metadata": {
        "id": "eAY6gv8RdffZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, n_outputs):\n",
        "        super(ViT, self).__init__()\n",
        "        self.n_outputs = n_outputs\n",
        "        # Define your Vision Transformer layers here"
      ],
      "metadata": {
        "id": "EC11nnWZdfF4"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss functions"
      ],
      "metadata": {
        "id": "rHm8wnbnGEnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a loss function class, or use a standart one."
      ],
      "metadata": {
        "id": "8z05aQ7cQTm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross entropy loss maden from scratch (just in case)\n",
        "class CELoss(nn.Module):\n",
        "    def __init__(self, reduction='mean'):\n",
        "        super(CELoss, self).__init__()\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Compute softmax probabilities\n",
        "        prob = nn.functional.softmax(x, 1)\n",
        "        # Compute log probabilities\n",
        "        log_prob = -1.0 * torch.log(prob)\n",
        "        # Gather the log probabilities for the true labels\n",
        "        loss = log_prob.gather(1, y.unsqueeze(1))\n",
        "        # Apply reduction\n",
        "        if self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif self.reduction == 'none':\n",
        "            loss = loss.squeeze()  # Remove extra dimension for consistency\n",
        "        else:\n",
        "            raise ValueError(\"Invalid reduction option.\")\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "8auVRUCKGEG2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLoss(nn.Module):\n",
        "    def __init__(self, label_smoothing=0.1, num_classes=NUM_CLASSES):\n",
        "        super(NLoss, self).__init__()\n",
        "        self.label_smoothing = label_smoothing\n",
        "        self.num_classes = num_classes\n",
        "        self.inv_smoothing = 1.0 - label_smoothing  # Probability for the correct class\n",
        "        self.smoothing = label_smoothing / (num_classes - 1)  # Probability for incorrect classes\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        x: Model output (logits + log variance)\n",
        "            - x[:, :self.num_classes]: Logits for class probabilities (h)\n",
        "            - x[:, self.num_classes:]: Logarithmic variance (s)\n",
        "        y: Labels\n",
        "        \"\"\"\n",
        "        # Split the model output into predictions (h) and log variance (s)\n",
        "        logits = x[:, :self.num_classes]  # Predictions (h)\n",
        "        log_var = x[:, self.num_classes:]  # Logarithmic variance (s)\n",
        "\n",
        "        # Apply label smoothing to the one-hot encoded labels\n",
        "        with torch.no_grad():\n",
        "            yoh = torch.zeros_like(logits)\n",
        "            yoh.fill_(self.smoothing / (self.num_classes - 1))\n",
        "            yoh.scatter_(1, y.data.unsqueeze(1), self.inv_smoothing)\n",
        "\n",
        "        # Compute the squared differences between predictions and smoothed labels\n",
        "        squared_diff = torch.pow(yoh - logits, 2)  # (y_k - h_k)^2\n",
        "\n",
        "        # Compute the exponential of the negative log variance (e^{-s})\n",
        "        exp_neg_log_var = torch.exp(-log_var)\n",
        "\n",
        "        # Compute the first term of the loss: e^{-s} * sum((y_k - h_k)^2)\n",
        "        term1 = exp_neg_log_var * squared_diff.sum(dim=1)\n",
        "\n",
        "        # Compute the second term of the loss: N * s\n",
        "        term2 = self.num_classes * log_var\n",
        "\n",
        "        # Combine the terms and compute the mean over the batch\n",
        "        loss = (term1 + term2).mean()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "An6Q9DUNK3SE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BLoss(nn.Module):\n",
        "    def __init__(self, label_smoothing=0.1, num_classes=NUM_CLASSES):\n",
        "        super(BLoss, self).__init__()\n",
        "        self.inv_smoothing = 1.0 - label_smoothing\n",
        "        self.smoothing = label_smoothing\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Extract certainty and probabilities from the model output\n",
        "        pass\n",
        "        #Enter your code here"
      ],
      "metadata": {
        "id": "99a0Eh3AK5wT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models zoo"
      ],
      "metadata": {
        "id": "ZHSuQd99s_Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architectures and loss functions"
      ],
      "metadata": {
        "id": "T0QP_le1tPL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arch_and_loss(hparams):\n",
        "    \"\"\"\n",
        "    Returns the architecture and loss function based on the provided hparams.\n",
        "\n",
        "    Args:\n",
        "        hparams (dict): Hyperparameters dictionary, including 'ARCHITECTURE' and 'criterion'.\n",
        "\n",
        "    Returns:\n",
        "        arch: The model architecture.\n",
        "        loss: The loss function.\n",
        "    \"\"\"\n",
        "    # Determine the number of outputs based on the loss function\n",
        "    if hparams['criterion'] in ['B', 'N']:\n",
        "        n_outputs = hparams['n_classes'] + 1  # Add 1 output neuron for BLoss or NLoss\n",
        "    else:\n",
        "        n_outputs = hparams['n_classes']  # Default number of outputs\n",
        "\n",
        "    # Define the architectures\n",
        "    architectures = {\n",
        "        'CNN': CNN(n_outputs=n_outputs),\n",
        "        'ResNet50': ResNet50(n_outputs=n_outputs),\n",
        "        'ViT': ViT(n_outputs=n_outputs),\n",
        "    }\n",
        "\n",
        "    # Define the loss functions\n",
        "    losses = {\n",
        "        'CE': nn.CrossEntropyLoss(),\n",
        "        'B': BLoss(),\n",
        "        'N': NLoss(),\n",
        "    }\n",
        "\n",
        "    # Get the architecture and loss based on hparams\n",
        "    arch = architectures.get(hparams['architecture'])\n",
        "    loss = losses.get(hparams['criterion'])\n",
        "\n",
        "    if arch is None:\n",
        "        raise ValueError(f\"Architecture '{hparams['ARCHITECTURE']}' is not supported.\")\n",
        "    if loss is None:\n",
        "        raise ValueError(f\"Loss function '{hparams['criterion']}' is not supported.\")\n",
        "\n",
        "    return arch, loss\n"
      ],
      "metadata": {
        "id": "WfLgyavidUcW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "CWrc-f3sWDDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(dataloader,model,hparams=hparams,loss_fn_red=None):\n",
        "    # Collect images, predictions, and losses\n",
        "    # images = []\n",
        "    preds  = []\n",
        "    labels = []\n",
        "    losses = []\n",
        "    correct= 0\n",
        "    total  = 0\n",
        "    for batch in dataloader:\n",
        "        x, y, _ = batch\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            # loss = loss_fn_red(h,y)\n",
        "            pred = torch.argmax(logits[:,:hparams['n_classes']], dim=1)\n",
        "        correct += (pred == y).sum().item()  # Number of correct predictions\n",
        "        total += y.size(0)  # Total number of samples\n",
        "\n",
        "        # images.extend(x.cpu())\n",
        "        preds.extend(pred.cpu().numpy())\n",
        "        labels.extend(y.cpu().numpy())\n",
        "        # losses.extend(loss.cpu().numpy())\n",
        "    acc = correct / total\n",
        "    return preds, labels, acc"
      ],
      "metadata": {
        "id": "7793mMFKWFYS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensembling\n",
        "This approach is expected to give a robust ensemble model that leverages the diversity introduced by different seeds, potentially improving the overall accuracy on the test set."
      ],
      "metadata": {
        "id": "KemJG62D7EJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataset and Data Loaders"
      ],
      "metadata": {
        "id": "7d0C987gzzlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of the dataset, the dataloader, and the training module"
      ],
      "metadata": {
        "id": "tQmBPmYH21V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here\n",
        "data_module = CIFAR10DataModule(hparams)"
      ],
      "metadata": {
        "id": "jrjw6mU-z36-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd8a69a-7092-4c62-d805-7ff2ed5a6978"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CIFAR-10_human.pt from GitHub...\n",
            "File downloaded and saved to ./data/CIFAR-10_human.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "qqGF7jmOGmhY",
        "outputId": "8f79b002-ac3d-48e6-84dd-bc4387d5d124"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.CIFAR10DataModule"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>CIFAR10DataModule</b><br/>def __init__(params)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>A DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is\n",
              "consistent data splits, data preparation and transforms across models.\n",
              "\n",
              "Example::\n",
              "\n",
              "    import lightning.pytorch as L\n",
              "    import torch.utils.data as data\n",
              "    from pytorch_lightning.demos.boring_classes import RandomDataset\n",
              "\n",
              "    class MyDataModule(L.LightningDataModule):\n",
              "        def prepare_data(self):\n",
              "            # download, IO, etc. Useful with shared filesystems\n",
              "            # only called on 1 GPU/TPU in distributed\n",
              "            ...\n",
              "\n",
              "        def setup(self, stage):\n",
              "            # make assignments here (val/train/test split)\n",
              "            # called on every process in DDP\n",
              "            dataset = RandomDataset(1, 100)\n",
              "            self.train, self.val, self.test = data.random_split(\n",
              "                dataset, [80, 10, 10], generator=torch.Generator().manual_seed(42)\n",
              "            )\n",
              "\n",
              "        def train_dataloader(self):\n",
              "            return data.DataLoader(self.train)\n",
              "\n",
              "        def val_dataloader(self):\n",
              "            return data.DataLoader(self.val)\n",
              "\n",
              "        def test_dataloader(self):\n",
              "            return data.DataLoader(self.test)\n",
              "\n",
              "        def on_exception(self, exception):\n",
              "            # clean up state after the trainer faced an exception\n",
              "            ...\n",
              "\n",
              "        def teardown(self):\n",
              "            # clean up state after the trainer stops, delete files...\n",
              "            # called on every process in DDP\n",
              "            ...</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Ensemble"
      ],
      "metadata": {
        "id": "oBfy6DZL0tii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop over different seeds"
      ],
      "metadata": {
        "id": "sjxsAXHbrGC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store predictions from each model\n",
        "all_predictions = []"
      ],
      "metadata": {
        "id": "4qZzt5w4smrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in SEEDS:\n",
        "    # Set seed for reproducibility at the VERY BEGINNING\n",
        "    pl.seed_everything(seed)\n",
        "\n",
        "    # Reinitialize the model architecture for each seed\n",
        "    #arch, loss_fn =  #Enter your code here\n",
        "\n",
        "\n",
        "    #checkpoint_callback_img = #Enter your code here\n",
        "\n",
        "    #task = #Enter your code here\n",
        "\n",
        "    # Initialize the model with the reinitialized architecture\n",
        "    # model = #Enter your code here\n",
        "\n",
        "    # Log hyperparameters to ClearML\n",
        "    task.connect(model.hparams)\n",
        "\n",
        "    #trainer = #Enter your code here\n",
        "\n",
        "    # best_model_path = #Enter your code here\n",
        "    task.update_output_model(model_path=best_model_path, auto_delete_file=False)\n",
        "    # best_model = #Enter your code here\n",
        "\n",
        "    # Test set\n",
        "    # test_dataloader = #Enter your code here\n",
        "\n",
        "    # Move the model to the correct device\n",
        "    # best_model = #Enter your code here\n",
        "    # predictions = #Enter your code here\n",
        "\n",
        "    if seed != SEEDS[-1]:\n",
        "        task.close()\n",
        "        del[model, best_model, task, arch, loss_fn]"
      ],
      "metadata": {
        "id": "QisO7wDarFVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the models and the ensemble of the models"
      ],
      "metadata": {
        "id": "NYbuTthVuCDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOYyF657_kOY",
        "outputId": "b4a11b84-d51d-4ac1-814d-baf0c83f3f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([5, 8, 8, ..., 5, 1, 7]),\n",
              " array([3, 1, 8, ..., 5, 1, 7]),\n",
              " array([5, 8, 8, ..., 5, 1, 7]),\n",
              " array([5, 8, 8, ..., 5, 1, 7]),\n",
              " array([5, 8, 8, ..., 5, 1, 7]),\n",
              " array([5, 8, 8, ..., 5, 1, 7]),\n",
              " array([3, 8, 8, ..., 5, 1, 7])]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual models"
      ],
      "metadata": {
        "id": "yRxEnWceGYxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store individual model accuracies\n",
        "individual_accuracies = []\n",
        "\n",
        "# Compute accuracy for each model\n",
        "for i, predictions in enumerate(all_predictions):\n",
        "    # Get predictions for the current model\n",
        "    model_predictions = predictions  # Shape: (num_samples,)\n",
        "\n",
        "    # Get true labels (already collected earlier)\n",
        "    true_labels = np.array(data_module.cifar10_test.targets)\n",
        "\n",
        "    # Calculate accuracy for the current model\n",
        "    accuracy = accuracy_score(true_labels, model_predictions)\n",
        "    individual_accuracies.append(accuracy)\n",
        "    print(f'Model {i+1} Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Convert to numpy array for easier calculations\n",
        "individual_accuracies = np.array(individual_accuracies)\n",
        "\n",
        "# Compute mean accuracy\n",
        "mean_accuracy = np.mean(individual_accuracies)\n",
        "\n",
        "# Compute standard deviation of accuracy\n",
        "std_accuracy = np.std(individual_accuracies)\n",
        "\n",
        "print(f'Mean Accuracy: {mean_accuracy:.4f}')\n",
        "print(f'Standard Deviation of Accuracy: {std_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSr0zvfIGeSd",
        "outputId": "3103f7dc-9d7a-4b41-f07c-948bbe0d3ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 Accuracy: 0.7183\n",
            "Model 2 Accuracy: 0.7062\n",
            "Model 3 Accuracy: 0.7123\n",
            "Model 4 Accuracy: 0.7151\n",
            "Model 5 Accuracy: 0.7116\n",
            "Model 6 Accuracy: 0.7174\n",
            "Model 7 Accuracy: 0.7141\n",
            "Mean Accuracy: 0.7136\n",
            "Standard Deviation of Accuracy: 0.0038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble"
      ],
      "metadata": {
        "id": "iop73XibGhUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack predictions from all models\n",
        "all_predictions = np.stack(all_predictions)  # Shape: (num_models, num_samples, num_classes)\n",
        "\n",
        "# Ensemble predictions (e.g., by averaging)\n",
        "ensemble_predictions = np.mean(all_predictions, axis=0)  # Shape: (num_samples, num_classes)\n",
        "final_predictions, _ = mode(all_predictions, axis=0)  # Majority voting\n",
        "final_predictions = final_predictions.flatten()  # Flatten to 1D array\n",
        "\n",
        "# Get true labels from the CIFAR-10 data set\n",
        "test_labels = np.array(data_module.cifar10_test.targets)\n",
        "# test_labels = data_module.test_dataset.labels  # Adjust this based on your dataset\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, final_predictions)\n",
        "print(f'Ensemble Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(test_labels, final_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyUFGexws8QK",
        "outputId": "68f320a5-18ce-4dca-ac40-7c8cbef4810c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Accuracy: 0.7426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated test metrics\n",
        "test_metrics = {\n",
        "    \"Mean Accuracy (individual)\": mean_accuracy,\n",
        "    \"Standard Deviation of Accuracy (individual)\": std_accuracy,\n",
        "    \"Ensemble Accuracy\": accuracy,\n",
        "}\n",
        "\n",
        "task.connect(test_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbEGs97IgtRD",
        "outputId": "d48caccb-bd1f-41dd-d558-a089ec725b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Mean Accuracy (individual)': 0.7135714285714287,\n",
              " 'Standard Deviation of Accuracy (individual)': 0.0037696965719932834,\n",
              " 'Ensemble Accuracy': 0.7426}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task.close()"
      ],
      "metadata": {
        "id": "nIqc-cspappI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}